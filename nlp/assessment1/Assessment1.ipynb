{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import trigrams,bigrams  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_all(sent): #using pos_tag to lemmatize words\n",
    "    word_lem = WordNetLemmatizer()\n",
    "    word_list=[]\n",
    "    for word, tag in pos_tag(word_tokenize(sent)):\n",
    "        if tag.startswith('N'): \n",
    "            wordnet_pos=wordnet.NOUN\n",
    "            word_list.append(word_lem.lemmatize(word, wordnet_pos))\n",
    "        elif tag.startswith('V'):\n",
    "             wordnet_pos=wordnet.VERB\n",
    "             word_list.append(word_lem.lemmatize(word, wordnet_pos))\n",
    "        elif tag.startswith('J'):\n",
    "             wordnet_pos=wordnet.ADJ\n",
    "             word_list.append(word_lem.lemmatize(word, wordnet_pos))\n",
    "        elif tag.startswith('R'):\n",
    "             wordnet_pos=wordnet.ADV\n",
    "             word_list.append(word_lem.lemmatize(word, wordnet_pos))\n",
    "        else:\n",
    "            wordnet_pos=wordnet.NOUN\n",
    "            word_list.append(word_lem.lemmatize(word, wordnet_pos))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn1=re.compile(r'[^\\s0-9A-Za-z]') # Remove all non-alphanumeric characters except spaces\n",
    "ptn2=re.compile(r'\\b\\w[1]\\b') # Remove words with only 1 character\n",
    "ptn3=re.compile(r'\\b\\d+\\b') # Remove numbers that are fully made of digits \n",
    "ptn4=re.compile(r'[A-Za-z]+://[^\\s]*')  # Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()# record start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"positive-words.txt\",\"r\") as t: # read positive-words.txt and store positive-words in a set\n",
    "    token_pos=Counter(t.read().split()) \n",
    "\n",
    "with open(r\"negative-words.txt\",\"r\") as t: # read negative-words.txt and store negative-words in a set\n",
    "    token_neg=Counter(t.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus contains 10400 positive stories\n",
      "this corpus contains 6941 negative stories\n",
      "this corpus contains 176494 positive words\n",
      "this corpus contains 143471 negative words\n"
     ]
    }
   ],
   "source": [
    "with open(r\"signal-news1.jsonl\", \"r\") as f: # open json file\n",
    "\n",
    "    total_pos=0 #  counter: calculate all positive-words\n",
    "    total_neg=0 #  counter: calculate all negative-words\n",
    "    pos_story=0 #  counter: calculate all positive-stories \n",
    "    neg_story=0 #  counter: calculate all negative-stories       \n",
    "    all_str=[]  #  counter: store words after using lemmatization\n",
    "\n",
    "    for line in f.readlines(): # using readlines because the file is too large, we cannot read all line in a time\n",
    "        pos_num=0 #  counter: calculate all positive-words in each story\n",
    "        neg_num=0 #  counter: calculate all negative-words in each story\n",
    "        dic=json.loads(line) \n",
    "        str1=dic['content']  # read the text in content       \n",
    "        str1=str1.lower() \n",
    "        str1 = re.sub(ptn4,\"\",str1) # replace URLs with \"\"\n",
    "        str1 = re.sub(ptn1,\"\",str1) # replace all non-alphanumeric characters except spaces with \"\"\n",
    "        str1 = re.sub(ptn3,\"\",str1) # replace numbers that are fully made of digits with \"\"\n",
    "        str1 = re.sub(ptn2,\"\",str1) # replace words with only 1 character with \"\"\n",
    "        lemt_list=lemmatize_all(str1) # lemt_list:store a new story after text preprocessing and lemmatization\n",
    "        all_str+=lemmatize_all(str1)  # all_str:store all new story after text preprocessing and lemmatization\n",
    "        for item in lemt_list: # record positive-words numbers and negative-words numbers in each story\n",
    "            pos_num+=token_pos[item]\n",
    "            neg_num+=token_neg[item]\n",
    "#        print(pos_num,neg_num)\n",
    "        if pos_num>neg_num:\n",
    "            pos_story+=1 # if a story with more positive than negative words, positive story number +1 ; vice versa\n",
    "        elif pos_num<neg_num: \n",
    "            neg_story+=1\n",
    "        total_pos+=pos_num  # total positive-words add positive-words in each stroy every time   \n",
    "        total_neg+=neg_num  # total neagtive-words add negative-words in each stroy every time\n",
    "    print(\"this corpus contains\" ,pos_story,\"positive stories\")\n",
    "    print(\"this corpus contains\" ,neg_story,\"negative stories\")\n",
    "    print(\"this corpus contains\" ,total_pos,\"positive words\")\n",
    "    print(\"this corpus contains\" ,total_neg,\"negative words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus contains 5883766 tokens and 123107 vocabularies.\n",
      "top 25 trigrams:  [('02jun16', '09jun16', '16jun16'), ('0749s', '290kmh', 'loris'), ('09jun16', '16jun16', '21jul16'), ('119x', '121x', '354x'), ('1250jkg1335', '1500ume1545', '1820dus'), ('16jun16', '21jul16', '28jul16'), ('1946s', 'gilda', '1935s'), ('1esrc', 'ufparentnodeinsertbeforee', 'fdocumentcreateelementscriptdocumentgetelementsbytagnamescript0'), ('1x8x', 'msaa', 'remappable'), ('21675s', 'aurelien', 'panis'), ('21jul16', '28jul16', '04aug16'), ('22120s', 'jazeman', 'jaafar'), ('282de', 's68', 'f52jkam'), ('28lq6q', 'ii28lq6q', '5clqecqmde'), ('29860s', '0019s', '180844mph'), ('29910s', '0069s', '180542mph'), ('29anthony', 'warlow', 'neverlands'), ('2bromo2nitropropane13diol', '5bromo5nitro13dioxane', 'hydroxymethylglycinate3'), ('30018s', '0177s', '179892mph'), ('30044s', '0203s', '179736mph'), ('30059s', '0218s', '179647mph'), ('30144s', '0004s', '179140mph'), ('30163s', '0023s', '179027mph'), ('30172s', '0331s', '178974mph'), ('30173s', '0033s', '178968mph')]\n"
     ]
    }
   ],
   "source": [
    "str3=\" \".join(all_str) # list to str   words_tokenize needs string\n",
    "n=len(all_str) # n is number of tokens\n",
    "v=len(set(all_str)) # v is vocabulary size\n",
    "print(\"This corpus contains\",n,\"tokens and\",v,\"vocabularies.\")\n",
    "tokens=nltk.word_tokenize(str3) \n",
    "finder=nltk.collocations.TrigramCollocationFinder.from_words(tokens) #discovering trinary phrases and sorting them, typically building a searcher using the function from_words()\n",
    "trigram_measures=nltk.collocations.TrigramAssocMeasures()\n",
    "top_25=finder.nbest(trigram_measures.pmi, 25) #Using point mutual information to calculate the score of each n-element phrase\n",
    "print(\"top 25 trigrams: \",top_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'this', 'a', 'good', 'thing', 'the', 'answer', 'is', 'no', 'longer']\n"
     ]
    }
   ],
   "source": [
    "with open(r\"signal-news1.jsonl\",\"r\") as f:\n",
    "    train_str=[] # store words on the first 16000 rows of corpus   \n",
    "    for line in f.readlines()[:16000]:\n",
    "        train_dic=json.loads(line)\n",
    "        train_str1=train_dic['content']        \n",
    "        train_str1=train_str1.lower()\n",
    "        train_str1 = re.sub(ptn4,\"\",train_str1) # preprocessing train sentences\n",
    "        train_str1 = re.sub(ptn1,\"\",train_str1) \n",
    "        train_str1 = re.sub(ptn3,\"\",train_str1)\n",
    "        train_str1 = re.sub(ptn2,\"\",train_str1)\n",
    "        train_str += train_str1.split()  \n",
    "  \n",
    "train_v=len(set(train_str)) # train_v is vocabulary size on the first 16000 rows\n",
    "train_str=\" \".join(train_str)\n",
    "tokens=nltk.word_tokenize(train_str)\n",
    "\n",
    "trigram_list=list(trigrams(tokens)) #using list store trigrams on the first 16000 rows\n",
    "trigram_count=Counter(trigram_list) # convert trigrams to Counter in order to speed up the running rate\n",
    "bigram_list=list(bigrams(tokens)) #using list store bigrams on the first 16000 rows\n",
    "bigram_count=Counter((bigram_list)) # convert bigrams to Counter in order to speed up the running rate\n",
    "new_word=['is','this'] # store whole sentences in new_word\n",
    "for i in range(8):  # loop 8 times to find 8 new words  \n",
    "    new_tri=[] #store matched trigram\n",
    "    for item in trigram_list: #find matched trigram\n",
    "        if item[0:2]==(new_word[i],new_word[i+1]):\n",
    "            new_tri.append(item)        \n",
    "    t= Counter(new_tri).most_common(1)[0] # find the most frequent matched trigram\n",
    "    nword=t[0][2] # read the third element in the most frequent matched trigram\n",
    "    new_word.append(nword) # add the third element in the most frequent matched trigram into list\n",
    "    i+=1\n",
    "print(new_word)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"signal-news1.jsonl\", \"r\") as f:\n",
    "    test_str=[] # store words after the first 16000 rows of corpus \n",
    "    test_trigram_list=[] # store trigrams after the first 16000 rows of corpus\n",
    "    for line in f.readlines()[16000:]: #\n",
    "        test_dic=json.loads(line)\n",
    "        str4=test_dic['content']        \n",
    "        str4=str4.lower()\n",
    "        for item in nltk.sent_tokenize(str4):   \n",
    "            str4 = re.sub(ptn4,\"\",item)\n",
    "            str4 = re.sub(ptn1,\"\",str4) \n",
    "            str4 = re.sub(ptn3,\"\",str4)\n",
    "            str4 = re.sub(ptn2,\"\",str4)\n",
    "            test_str += str4.split()\n",
    "            if len(str4.split())<3: # ignore sentences which are less than 3 words\n",
    "                continue\n",
    "            test_tokens=nltk.wordpunct_tokenize(str4)\n",
    "            test_trigram_list+=list(trigrams(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42090.347733125025\n"
     ]
    }
   ],
   "source": [
    "p=0 #using for perplexity computation\n",
    "test_len=len(test_trigram_list) # the number of trigrams after the first 16000 rows of corpus\n",
    "for item in test_trigram_list: \n",
    "    if item in trigram_count and item[0:2] in bigram_count: # calculate the occurrences of trigrams and corresponding bigrams\n",
    "        p=p+math.log((trigram_count[item]+1)/(bigram_count[item[0:2]]+train_v),2)\n",
    "    else:\n",
    "        p=p+math.log((1/train_v),2)\n",
    "\n",
    "plex=pow(2,-p/test_len) #perplexity computation\n",
    "print(plex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time is 894.903501033783 second\n"
     ]
    }
   ],
   "source": [
    "end = time.time() #record end time\n",
    "print (\"running time is\",end-start,\"second\") #print running time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
